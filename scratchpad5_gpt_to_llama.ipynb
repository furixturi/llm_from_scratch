{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT to Llama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GPT-2 to Llama 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Differences between GPT-2 and Llama 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 and Llama 2 has very similar architecture. The differences are:\n",
    "- Normalization layer: LayerNorm -> **RMSNorm**\n",
    "  - Instead of LayerNorm in GPT-2 (both the ones in the transformer blocks and the last LayerNorm before output layer), Llama 2 uses `RMSNorm` (**Root Mean Square Layer Normalization**) layer. While LayerNorm normalizes inputs using mean and variance, RMSNorm uses only the root mean square, which improves computational efficiency.\n",
    " \n",
    "- FeedForward layer's activation function: GELU -> **SwiGLU**\n",
    "  - In the FeedForward part, to introduce non-linearity, instead of using GELU activation like GPT-2, Llama 2 uses `SwiGLU` (a variant of the **Gated Linear Unit(GLU)** that incorporates the `SiLU`(also known as `Swish`) function.) SwiGLU has been shown to improve performance in Transformer architectures, outperforming traditional activation functions like ReLU and GELU in various tasks. \n",
    " \n",
    "- Positional embedding: absolute positional embedding -> **RoPE**\n",
    "  - Instead of the traditional absolute positional embeddings in GPT-2, Llama 2 uses **rotary position embeddings**(`RoPE`), which enables it to capture both absolute and relative positional information simultaneously.\n",
    "\n",
    "- Llama 2 doesn't use the Dropout layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama 2 uses Google's [SentencePiece](https://github.com/google/sentencepiece) tokenizer, not OpenAI's [Tiktoken](https://github.com/openai/tiktoken) (but Llama 3 uses Tiktoken). Both are **Byte Pair Encoding (BPE)** tokenizers and Tiktoken offers improved efficiency and flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Convert GPT-2 to Llama 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Replace LayerNorm with RMSNorm layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast to LayerNorm which uses inputs' mean and variance to normalize, RMSNorm only uses root mean square and improves computational efficiency. The paper: [Root Mean Square Layer Normalization](https://arxiv.org/abs/1910.07467)\n",
    "\n",
    "The RMSNorm operation is as follows, where $x$ is the input and $\\gamma$ is a trainable parameter vector. $\\epsilon$ is a small constant to avoid zero-division errors:\n",
    "\n",
    "$$ y_i = \\frac{x_i}{\\text{RMS}(x)} \\gamma_i, \\quad \\text{where} \\quad \\text{RMS}(x) = \\sqrt{\\epsilon + \\frac{1}{n} \\sum x_i^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Use this to replace the class LayerNorm\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, emb_dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.emb_dim = emb_dim\n",
    "        self.weight = nn.Parameter(torch.ones(emb_dim)).float()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        means = x.pow(2).mean(dim=-1, keepdim=True)\n",
    "        x_normed = x * torch.rsqrt(means + self.eps)\n",
    "        return (x_normed * self.weight).to(dtype=x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Replace GELU with SwiGLU in FeedForward module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2.1 Replace GELU with SwiGLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2.2 Update the FeedForward module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Implement RoPE for positional embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.3.1 Implement RoPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.3.2 Add RoPE to MultiHeadAttention module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.4 Update the TransformerBlock module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.5 Update the model class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Load model and pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Initialize the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Load tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.3 Load pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Use the pretrained Llama 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 2 to Llama 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
