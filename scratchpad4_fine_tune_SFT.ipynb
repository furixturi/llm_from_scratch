{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad 4 - Instruction Fine-tuning (SFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why instruction fine-tuning? A pretrained LLM is only good at text completion (predicting the next token), not good at following instructions. With instruction fine-tuning, we teach LLM to better follow instructions (generating texts that are desirable responses to the user's instructions). This is often called \"supervised fine-tuning (SFT)\".\n",
    "\n",
    "For SFT, we need to\n",
    "1. prepare instruction fine-tuning data that has desired input output pairs\n",
    "2. load and fine-tune a pretrained LLM\n",
    "3. evaluate the fine-tuned LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare data for supervised instruction fine-tuning, we need to:\n",
    "1. Get the raw dataset and format it according to the SFT template\n",
    "2. Create an SFT DataSet that formats json data into prompt template and tokenizes the texts into token IDs \n",
    "3. Create the DataLoader for the custom SFT DataSet which uses a custom collate function to prepare input and target batches of the encoded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 SFT Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is stored in a JSON file in `./data/sft/train`. Each sample JSON data has three fields:  `instruction`, `input` , and `output`.\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"instruction\": \"Evaluate the following phrase by transforming it into the spelling given.\",\n",
    "        \"input\": \"freind --> friend\",\n",
    "        \"output\": \"The spelling of the given phrase \\\"freind\\\" is incorrect, the correct spelling is \\\"friend\\\".\"\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "The test data for evaluation later will add the fine-tuned model's response for comparison. The file will be prepared in `./data/sft/test` and each data record will have an additional `response` field, e.g.:\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"instruction\": \"Rewrite the sentence using a simile.\",\n",
    "        \"input\": \"The car is very fast.\",\n",
    "        \"output\": \"The car is as fast as lightning.\",\n",
    "        \"model_response\": \"The car is as fast as a bullet.\"\n",
    "    },\n",
    "    ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 LLM prompt templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare the train data into inputs to the LLM. \n",
    "\n",
    "There are two example prompt template formats:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Alpaca\n",
    "which Stanford CRFM used to train [Stanford Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)\n",
    "  - They released 52K instruction-following data and the generation script in their [repo](https://github.com/tatsu-lab/stanford_alpaca#data-release). Our training data JSON is in exactly the same JSON format.\n",
    "  - The LLM prompt template reflects this format:\n",
    "\n",
    "With the input field:\n",
    "\n",
    "```\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "```\n",
    "Without the input field:\n",
    "```\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Phi-3\n",
    "\n",
    "which Microsoft Research used to train [Phi-3](https://github.com/microsoft/Phi-3CookBook/tree/main). \n",
    "\n",
    "The Phi-3 training data prompt template is as follows:\n",
    "\n",
    "```\n",
    "<|system|>\n",
    "Your Role<|end|>\n",
    "<|user|>\n",
    "Your Question?<|end|>\n",
    "<|assistant|>\n",
    "```\n",
    "\n",
    "A `jsonl` data would be like follows:\n",
    "\n",
    "```json\n",
    "{\"text\": \"<|user|>\\nWhen were iron maidens commonly used? <|end|>\\n<|assistant|> \\nIron maidens were never commonly used <|end|>\"}\n",
    "```\n",
    "\n",
    "But when using Azure AI to fine-tune, the data format is aligned with GPT, i.e.:\n",
    "\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are an Xbox customer support agent whose primary goal is to help users with issues they are experiencing with their Xbox devices. You are friendly and concise. You only provide factual answers to queries, and do not provide answers that are not related to Xbox.\"}, {\"role\": \"user\", \"content\": \"Is Xbox better than PlayStation?\"}, {\"role\": \"assistant\", \"content\": \"I apologize, but I cannot provide personal opinions. My primary job is to assist you with any issues related to your Xbox device. Do you have any Xbox-related issues that need addressing?\"}]}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the Alpaca prompt template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Load and format data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the training data\n",
    "\n",
    "import json\n",
    "data_file = './data/sft/train/instruction-data.json'\n",
    "with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training data JSON lines: 1100\n",
      "Example:\n",
      "{\n",
      "    \"instruction\": \"Evaluate the following phrase by transforming it into the spelling given.\",\n",
      "    \"input\": \"freind --> friend\",\n",
      "    \"output\": \"The spelling of the given phrase \\\"freind\\\" is incorrect, the correct spelling is \\\"friend\\\".\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "f\"\"\"number of training data JSON lines: {len(data)}\n",
    "Example:\\n{json.dumps(data[0], indent=4)}\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format it according to prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the data into prompt template for input to LLM\n",
    "\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task.\"\n",
    "        f\"Write a response that appropriately completes the task.\"\n",
    "        f\"\\n\\n### Instruction\\n{entry['instruction']}\"\n",
    "    )\n",
    "    input_text = f\"\\n\\n### Input\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    return instruction_text + input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task.Write a response that appropriately completes the task.\n",
      "\n",
      "### Instruction\n",
      "Evaluate the following phrase by transforming it into the spelling given.\n",
      "\n",
      "### Input\n",
      "freind --> friend\n",
      "\n",
      "### Response:\n",
      "The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".\n"
     ]
    }
   ],
   "source": [
    "model_input = format_input(data[0])\n",
    "desired_output = f\"\\n\\n### Response:\\n{data[0]['output']}\"\n",
    "\n",
    "print(model_input + desired_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 Split training, validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_portion = int(len(data) * 0.85) # 85% of the data for training\n",
    "test_portion = int(len(data) * 0.1) # 10% of the data for testing\n",
    "val_portion = len(data) - train_portion - test_portion # 5% of the data for validation\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion + test_portion]\n",
    "val_data = data[train_portion + test_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 935, test: 110, val: 55\n"
     ]
    }
   ],
   "source": [
    "print(f\"train: {len(train_data)}, test: {len(test_data)}, val: {len(val_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create custom DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a custom DataSet that at initialization: \n",
    "- format json data into LLM prompt template\n",
    "- tokenize the text into token IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.encoded_texts = []\n",
    "\n",
    "        for entry in data:\n",
    "            # format the data input and response into prompt template\n",
    "            instruction_plus_input = (\n",
    "                f\"Below is an instruction that describes a task.\"\n",
    "                f\"Write a response that appropriately completes the task.\"\n",
    "                f\"\\n\\n### Instruction\\n{entry['instruction']}\"\n",
    "            ) + (f\"\\n\\n### Input\\n{entry['input']}\" if entry[\"input\"] else \"\")\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "\n",
    "            # tokenize the full text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encoded_texts[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## training data json ########\n",
      "training json data length: 935\n",
      "first training data json: {\n",
      "    \"instruction\": \"Evaluate the following phrase by transforming it into the spelling given.\",\n",
      "    \"input\": \"freind --> friend\",\n",
      "    \"output\": \"The spelling of the given phrase \\\"freind\\\" is incorrect, the correct spelling is \\\"friend\\\".\"\n",
      "}\n",
      "\n",
      "######## training dataset ########\n",
      "training dataset size: 935\n",
      "first item of the training dataset: \n",
      "[21106, 318, 281, 12064, 326, 8477, 257, 4876, 13, 16594, 257, 2882, 326, 20431, 32543, 262, 4876, 13, 198, 198, 21017, 46486, 198, 36, 2100, 4985, 262, 1708, 9546, 416, 25449, 340, 656, 262, 24993, 1813, 13, 198, 198, 21017, 23412, 198, 19503, 521, 14610, 1545, 198, 198, 21017, 18261, 25, 198, 464, 24993, 286, 262, 1813, 9546, 366, 19503, 521, 1, 318, 11491, 11, 262, 3376, 24993, 318, 366, 6726, 1911]\n",
      "Decoded first item of the training dataset: \n",
      "Below is an instruction that describes a task.Write a response that appropriately completes the task.\n",
      "\n",
      "### Instruction\n",
      "Evaluate the following phrase by transforming it into the spelling given.\n",
      "\n",
      "### Input\n",
      "freind --> friend\n",
      "\n",
      "### Response:\n",
      "The spelling of the given phrase \"freind\" is incorrect, the correct spelling is \"friend\".\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "\n",
    "print(\"######## training data json ########\")\n",
    "print(f\"training json data length: {len(train_dataset)}\")\n",
    "print(f\"first training data json: {json.dumps(train_data[0], indent=4)}\")\n",
    "print(\"\\n######## training dataset ########\")\n",
    "print(f\"training dataset size: {len(train_dataset)}\")\n",
    "print(f\"first item of the training dataset: \\n{train_dataset[0]}\")\n",
    "print(f\"Decoded first item of the training dataset: \\n{tokenizer.decode(train_dataset[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create DataLoader with a custom colloate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.1 Custom collate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we develop a custom collate function for batching, which\n",
    "- pad items in each batch according to the longest sequence in that batch\n",
    "- prepare input and target pairs of each sample in the batch\n",
    "- mask padding tokens so that they are ignored in training\n",
    "- optionally truncate sequences according to a given model context window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.get_device import get_default_device\n",
    "\n",
    "def custom_collate_fn(\n",
    "    batch,\n",
    "    pad_token_id=50256,  # default pad token is GPT2's end of text token <|endoftext|>\n",
    "    ignore_token_index=-100,  # PyTorch ignores token ID -100 in calculating loss\n",
    "    allowed_max_length=None,\n",
    "    device=None,\n",
    "):\n",
    "    # pad items in the batch to the same length as the longest sequence in this batch\n",
    "    batch_max_length = max(len(item) + 1 for item in batch)\n",
    "    inputs_batch_list, targets_batch_list = [], []\n",
    "    for item in batch:\n",
    "        # first, add an <endoftext> token to the end of the sequence\n",
    "        new_item = item.copy() + [pad_token_id]\n",
    "        # pad the sequence to the same length as the longest sequence in the batch\n",
    "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
    "\n",
    "        # preoare inputs and outputs with the padded sequence\n",
    "        inputs = torch.tensor(padded[:-1]) # remove the last token for inputs\n",
    "        targets = torch.tensor(padded[1:]) # shift input +1 to the right for targets\n",
    "\n",
    "        # mask the padding tokens in targets with the ignore token so that they don't affect training\n",
    "        mask = targets == pad_token_id\n",
    "        indices = torch.nonzero(mask).squeeze()\n",
    "        if indices.numel() > 1: \n",
    "            targets[indices[1:]] = ignore_token_index # keep the first padding token because it shows the end of the sequence\n",
    "\n",
    "        # if given an allowed max sequence length, truncate both inputs and targets sequences\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        # add prepared inputs and outputs tokens to the corresponding batch list\n",
    "        inputs_batch_list.append(inputs)\n",
    "        targets_batch_list.append(targets)\n",
    "    \n",
    "    # stack inputs and targets batch lists into batch tensors and move them to device\n",
    "    if device is None:\n",
    "        device = get_default_device()\n",
    "    inputs_tensor = torch.stack(inputs_batch_list).to(device)\n",
    "    targets_tensor = torch.stack(targets_batch_list).to(device)\n",
    "\n",
    "\n",
    "    return inputs_tensor, targets_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:tensor([[    0,     1,     2,     3],\n",
      "        [    5,     6, 50256, 50256],\n",
      "        [    7,     8,     9, 50256]], device='mps:0')\n",
      "targets:tensor([[    1,     2,     3,     4],\n",
      "        [    6, 50256,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "inputs_1 = [0, 1, 2, 3, 4]\n",
    "inputs_2 = [5, 6]\n",
    "inputs_3 = [7, 8, 9]\n",
    "\n",
    "batch = (\n",
    "    inputs_1,\n",
    "    inputs_2,\n",
    "    inputs_3\n",
    ")\n",
    "\n",
    "inputs, targets = custom_collate_fn(batch, allowed_max_length=4)\n",
    "print(f\"inputs:{inputs}\\ntargets:{targets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3.2 Train, test, validation DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train, test and validation dataloaders will use the custom dataset and collate function we defined. \n",
    "\n",
    "The custom collate function has a `device` argument, so before we pass this function to create the DataLoader, we want to first give it the correct `device`. We can use Python's `functools` library's `partial` function to create a new function of it with `device` argument pre-filled. We can also prefill the `allowed_max_length` to GPT2's context window 1024. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.get_device import get_default_device\n",
    "device = get_default_device()\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the dataloaders for train, test, and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Finetuning a pretrained LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fine-tune a pretrained LLM, we need to\n",
    "1. load the pretrained LLM\n",
    "2. train (fine-tune) it on SFT data\n",
    "3. save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluating the fine-tuned LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the fine-tuned LLM, we will use another LLM.\n",
    "1. run inference on test set and save the responses\n",
    "2. compare ground truth in test set with generated responses\n",
    "3. use another LLM to score the fine-tuned responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
