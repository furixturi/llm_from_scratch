{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad 3 - Pre-train LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calculate text generation loss: cross-entropy and perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Cross-entropy loss (average negative log probability value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model generates logits tensor of (batch_size, seq_length, vocab_size), the last dimension can be interpreted as the mode's predicted probability of each token in the whole vacabulary of being the next token. Applying `softmax` makes them probability values from 0 to 1, sumning up to 1.\n",
    "\n",
    "For the target tokens (the input tokens minus the first token and plus the real next token in the training data), we want to maximize their logits in the output tokens. Which means we want to\n",
    "-  **maximize the target tokens' softmax-ed values in the ouput tensor to 1**. \n",
    "\n",
    "It's easier to do this if we first calculate their log values, because to max the probabilities to 1, then we just need to maximize their log values (which are negative numbers) to 0. \n",
    "\n",
    "In ML we need to define a loss function and minimize one loss value calculated from the function. So instead of maximizing muliple log values of all the batches' ouput token sequences, we define the loss to be the \n",
    "- **averaged negative log probability value** of all target tokens in the output tensor, from all batches in the output concatenated. \n",
    "\n",
    "This is our **cross-entropy** loss. PyTorch provides a `cross_entropy` function which combines two operations:\n",
    "- apply softmax \n",
    "- calculate log of softmax\n",
    "- calculate negative log-likelihood loss\n",
    "- average loss across batches\n",
    "\n",
    "To use it, we need to flatten the logits tensor and the target token IDs batch tensor (aka concat all batches):\n",
    "\n",
    "```\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, target_idx_batch_flat)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt.gpt_model import GPTModel\n",
    "import torch\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"embedding_dim\": 768,  # Embedding dimension\n",
    "    \"n_heads\": 12,  # Number of attention heads\n",
    "    \"n_layers\": 12,  # Number of layers\n",
    "    \"dropout_rate\": 0.1,  # Dropout rate\n",
    "    \"qkv_bias\": False,  # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval(); # ; is used to suppress the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full text tokens: [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290]\n"
     ]
    }
   ],
   "source": [
    "full_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "full_text_tokens = tokenizer.encode(full_text)\n",
    "print(f\"Full text tokens: {full_text_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: The quick brown fox\n",
      "Input idx batch tensor: tensor([[  464,  2068,  7586, 21831]])\n"
     ]
    }
   ],
   "source": [
    "input_idx = full_text_tokens[:4]\n",
    "input_text = tokenizer.decode(input_idx)\n",
    "print(f\"Input text: {input_text}\")\n",
    "input_idx_batch = torch.tensor(input_idx).unsqueeze(0)\n",
    "print(f\"Input idx batch tensor: {input_idx_batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output logits shape: torch.Size([1, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(input_idx_batch)\n",
    "print(f\"Output logits shape: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target text:  quick brown fox jumps\n",
      "Target idx batch tensor: tensor([[ 2068,  7586, 21831, 18045]])\n"
     ]
    }
   ],
   "source": [
    "target_idx = full_text_tokens[1:5]\n",
    "target_text = tokenizer.decode(target_idx)\n",
    "print(f\"Target text: {target_text}\")\n",
    "target_idx_batch = torch.tensor(target_idx).unsqueeze(0)\n",
    "print(f\"Target idx batch tensor: {target_idx_batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7479)\n"
     ]
    }
   ],
   "source": [
    "# logits is the model's output of shape (batch_size, seq_len, vocab_size) -> (batch_size * seq_len, vocab_size)\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "# targets is the target sequence's token IDs shape (batch_size, seq_len) -> (batch_size * seq_len)\n",
    "targets_idx_flat = target_idx_batch.flatten(0, 1)\n",
    "# Compute the cross entropy loss\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_idx_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our randomly initialized GPT2 model, the cross-entropy loss is 10.7479."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perplexity** is also a metric that measures how well the model predicts a sequence of tokens. It is the **exponential of the average negaltive log-likelihood** of the predicted probabilities for each token in a sequence, aka\n",
    "- the **exponential of cross-entropy**\n",
    "  \n",
    "It tells us how \"surprised\" the model is by the test data. It can be interpreted as how many choices the model thinks it has when predicting the next token.\n",
    "\n",
    "$$\\text{Perplexity} = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(x_i)\\right)$$\n",
    "\n",
    "```\n",
    "cros_entropy_loss = torch.nn.functional.cross_entropy(logits_flat, target_idx_batch_flat)\n",
    "perplexity_loss = torch.exp(cros_entropy_loss)\n",
    "```\n",
    "\n",
    "- A perplexity of 1 means the model is extremely sure (assigns probability of 1 to the correct next token ID).\n",
    "- A high perplexity means the model is uncertain about the next token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(46531.8984)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our untrained model has perplexity of 46532, with the vocabulary size 50257, it is basically random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a small dataset first. Download the \"verdict.txt\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# download file from url to destination, showing progress bar\n",
    "def download_file(url, destination):\n",
    "    # Create the destination directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(destination), exist_ok=True)\n",
    "    # Send a GET request to download the file in streaming mode\n",
    "    response = requests.get(url, stream=True)\n",
    "\n",
    "    # Get the total file size from headers, defaulting to 0 if not present\n",
    "    file_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "    # Check if file exists and has the same size\n",
    "    if os.path.exists(destination):\n",
    "        file_size_local = os.path.getsize(destination)\n",
    "        if file_size == file_size_local:\n",
    "            print(f\"File already exists and is up-to-date: {destination}\")\n",
    "            return\n",
    "\n",
    "    # Define the block size for reading the file\n",
    "    block_size = 1024  # 1 Kilobyte\n",
    "\n",
    "    # Initialize the progress bar with total file size\n",
    "    progress_bar_description = url.split(\"/\")[-1]  # Extract filename from URL\n",
    "    with tqdm(\n",
    "        total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description\n",
    "    ) as progress_bar:\n",
    "        # Open the destination file in binary write mode\n",
    "        with open(destination, \"wb\") as file:\n",
    "            # Iterate over the file data in chunks\n",
    "            for chunk in response.iter_content(block_size):\n",
    "                progress_bar.update(len(chunk))  # Update progress bar\n",
    "                file.write(chunk)  # Write the chunk to the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "the-verdict.txt: 20.5kiB [00:00, 15.5MiB/s]                  \n"
     ]
    }
   ],
   "source": [
    "data_folder = \"data\"\n",
    "file_path = \"the-verdict.txt\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "\n",
    "destination = os.path.join(data_folder, file_path)\n",
    "download_file(url, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n",
      " rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would h\n",
      "it for me! The Strouds stand alone, and happen once--but there's no exterminating our kind of art.\"\n"
     ]
    }
   ],
   "source": [
    "with open(destination, 'r', encoding='utf-8') as file:\n",
    "    text_data = file.read()\n",
    "print(text_data[:99])\n",
    "print(text_data[200:299])\n",
    "print(text_data[-99:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters: 20479\n",
      "Total number of tokens: 5145\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "total_tokens = tokenizer.encode(text_data)\n",
    "print(f\"Total characters: {len(text_data)}\")\n",
    "print(f\"Total number of tokens: {len(total_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Create custom dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a custom DataSet which extends PyTorch's `Dataset` to prepare an LLM training dataset, which\n",
    "- tokenizes the text dataset with a given tokenizer \n",
    "- chunks the text dataset into multiple chunks of max_length, with the specified stride\n",
    "- using a sliding window when chunking, so that\n",
    "  - input is a sequence of token ids\n",
    "  - target is one token slided right of the input token id sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use this custom dataset in PyTorch's `DataLoader`, which takes care of data batching, shuffling, parallel data loading, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With those prepared, let's split the dataset into training and validation at ratio 9:1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's create the dataloaders for training and validation datasets. \n",
    "- For faster training, we change the model config's context_length from 1024 to 256\n",
    "- We set `drop_last=True` in the datalodaer so that if there are not enough samples left to form a full batch at the end of the dataset, we drop this smaller final batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Vocabulary size\n",
    "    \"context_length\": 256,  # Context length !! Let's use 256 context length for faster training\n",
    "    \"embedding_dim\": 768,  # Embedding dimension\n",
    "    \"n_heads\": 12,  # Number of attention heads\n",
    "    \"n_layers\": 12,  # Number of layers\n",
    "    \"dropout_rate\": 0.1,  # Dropout rate\n",
    "    \"qkv_bias\": False,  # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check if the data is enough to cover at least context length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(total_tokens) * (train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the training loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"increase the `training_ratio`\")\n",
    "\n",
    "if len(total_tokens) * (1-train_ratio) < GPT_CONFIG_124M[\"context_length\"]:\n",
    "    print(\"Not enough tokens for the validation loader. \"\n",
    "          \"Try to lower the `GPT_CONFIG_124M['context_length']` or \"\n",
    "          \"decrease the `training_ratio`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that data was loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that token size is in the expected ballpark (we droped the last smaller batch so it is smaller than the full dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4608\n",
      "Validation tokens: 512\n",
      "All tokens: 5120\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Calculate loss batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implement two utils functions to calculate `cross-entropy` loss. \n",
    "\n",
    "One is for a given batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other calculates loss for multiple batches in a dataloder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches = None):\n",
    "    total_loss = 0\n",
    "    if len(data_loader) == 0:\n",
    "        return float('nan')\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "   device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "   device = torch.device(\"mps\")\n",
    "else:\n",
    "   device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(MPS seems to get faster the second time round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU : --- 0.27613019943237305 seconds ---\n",
      "MPS : --- 0.00039196014404296875 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "device = \"mps\"\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "TENSOR_A_CPU = torch.rand(5000, 5000)\n",
    "TENSOR_B_CPU = torch.rand(5000, 5000)\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "TENSOR_A_MPS = torch.rand(5000, 5000).to(device)\n",
    "TENSOR_B_MPS = torch.rand(5000, 5000).to(device)\n",
    "\n",
    "# Warm-up\n",
    "for _ in range(100):\n",
    "    torch.matmul(torch.rand(500,500).to(device), torch.rand(500,500).to(device))\n",
    "    \n",
    "start_time = time.time()\n",
    "torch.matmul(TENSOR_A_CPU, TENSOR_B_CPU)\n",
    "print(\"CPU : --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "start_time = time.time()\n",
    "torch.matmul(TENSOR_A_MPS, TENSOR_B_MPS)\n",
    "print(\"MPS : --- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the dataset's loss on an untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt.gpt_model import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Vocabulary size\n",
    "    \"context_length\": 256,  # Context length !! Let's use 256 context length for faster training\n",
    "    \"embedding_dim\": 768,  # Embedding dimension\n",
    "    \"n_heads\": 12,  # Number of attention heads\n",
    "    \"n_layers\": 12,  # Number of layers\n",
    "    \"dropout_rate\": 0.1,  # Dropout rate\n",
    "    \"qkv_bias\": False,  # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial train loss: 11.0054654015435105\n",
      "Initial validation loss: 11.0061912536621094\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "with torch.no_grad(): # disable gradient tracking for efficiency as we're not yet training\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(f\"Initial train loss: {train_loss:.16f}\")\n",
    "print(f\"Initial validation loss: {val_loss:.16f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Train and eval scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs,\n",
    "    eval_freq,\n",
    "    eval_iter,\n",
    "    start_context,\n",
    "    tokenizer,\n",
    "):\n",
    "    # lists to track losses and tokens seen\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen, global_step = 0, -1\n",
    "\n",
    "    # main training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # set model to training mode\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()  # reset loss gradients from previous batch iteration\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()  # calculate gradients\n",
    "            optimizer.step()  # update model weights\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # intermediate evaluation\n",
    "            if global_step % eval_freq == 0:\n",
    "                model.eval()  # set model to evaluation mode\n",
    "                with torch.no_grad():\n",
    "                    train_loss = calc_loss_loader(\n",
    "                        train_loader, model, device, eval_iter\n",
    "                    )\n",
    "                    val_loss = calc_loss_loader(val_loader, model, device, eval_iter)\n",
    "                    train_losses.append(train_loss)\n",
    "                    val_losses.append(val_loss)\n",
    "                    track_tokens_seen.append(tokens_seen)\n",
    "                    print(\n",
    "                        f\"Epoch: {epoch}, Global step: {global_step}, \"\n",
    "                        f\"Tokens seen: {tokens_seen}, Train loss: {train_loss:.4f}, \"\n",
    "                        f\"Validation loss: {val_loss:.4f}\"\n",
    "                    )\n",
    "                model.train()\n",
    "        # print a samplet text after each epoch\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    return train_losses, val_losses, track_tokens_seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every epoch we print a text generation to see if the model is getting better. \n",
    "\n",
    "The `generate_and_print_sample` utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.generate_tokens import generate_tokens_greedy\n",
    "import torch\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded_tensor = torch.tensor(tokenizer.encode(start_context)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_tokens_greedy(\n",
    "            model=model,\n",
    "            input_idx_batch=encoded_tensor,\n",
    "            max_new_tokens=50,\n",
    "            context_size=context_size,\n",
    "        )\n",
    "    decoded_text = tokenizer.decode(token_ids.squeeze(0).tolist())\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the LLM using the above training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Global step: 0, Tokens seen: 512, Train loss: 10.0420, Validation loss: 10.0227\n",
      "Epoch: 0, Global step: 5, Tokens seen: 3072, Train loss: 8.1697, Validation loss: 8.3290\n",
      "Every effort moves you,,,,, the,,,,,,, the,,,,,,,,,,,,,,,,,,,,,,,, the,,,,,,,,,,,\n",
      "Epoch: 1, Global step: 10, Tokens seen: 5632, Train loss: 6.7131, Validation loss: 7.0844\n",
      "Epoch: 1, Global step: 15, Tokens seen: 8192, Train loss: 6.1807, Validation loss: 6.6203\n",
      "Every effort moves you, and, and, the, and, and, and, the, and, and, and, and, and, and, and, the, and, and, and, and, and, and, and,, and, the,\n",
      "Epoch: 2, Global step: 20, Tokens seen: 10752, Train loss: 5.6658, Validation loss: 6.5348\n",
      "Epoch: 2, Global step: 25, Tokens seen: 13312, Train loss: 5.6759, Validation loss: 6.4869\n",
      "Every effort moves you .                                                 \n",
      "Epoch: 3, Global step: 30, Tokens seen: 15872, Train loss: 5.2342, Validation loss: 6.4239\n",
      "Epoch: 3, Global step: 35, Tokens seen: 18432, Train loss: 5.0158, Validation loss: 6.4387\n",
      "Every effort moves you.  \"  \"   \"I, and my, and, and I was, and I had. \"--I, and his, and the. I had, and the, and my, and the, and\n",
      "Epoch: 4, Global step: 40, Tokens seen: 20992, Train loss: 4.5761, Validation loss: 6.3260\n",
      "Every effort moves you.            \"I--I to my work, and my I had been to me, I was his a had been the a had the first, I had been--I had the first his\n",
      "Epoch: 5, Global step: 45, Tokens seen: 23552, Train loss: 4.3395, Validation loss: 6.2687\n",
      "Epoch: 5, Global step: 50, Tokens seen: 26112, Train loss: 3.6068, Validation loss: 6.1634\n",
      "Every effort moves you know that he had been the picture--I turned, and I feltI turned.                                 \n",
      "Epoch: 6, Global step: 55, Tokens seen: 28672, Train loss: 3.2385, Validation loss: 6.1704\n",
      "Epoch: 6, Global step: 60, Tokens seen: 31232, Train loss: 3.1312, Validation loss: 6.1857\n",
      "Every effort moves you know that, and a little of the Riv he was a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, I had dropped his painting, I had been the Riv, and I had\n",
      "Epoch: 7, Global step: 65, Tokens seen: 33792, Train loss: 2.4549, Validation loss: 6.1865\n",
      "Epoch: 7, Global step: 70, Tokens seen: 36352, Train loss: 2.1967, Validation loss: 6.1622\n",
      "Every effort moves you know,\" was one of the picture--I--I told Mrs.                                    \n",
      "Epoch: 8, Global step: 75, Tokens seen: 38912, Train loss: 1.7021, Validation loss: 6.2075\n",
      "Epoch: 8, Global step: 80, Tokens seen: 41472, Train loss: 1.2627, Validation loss: 6.2705\n",
      "Every effort moves you know,\" was one of the axioms he laid down across the Sevres and silver of an exquisburn's an unusual degree to the display of the his glory, the donkey.  \"Oh, when Stroud laid in the first\n",
      "Epoch: 9, Global step: 85, Tokens seen: 44032, Train loss: 1.0119, Validation loss: 6.2917\n",
      "Every effort moves you?\"  \"Yes--I glanced after him, and Mrs.  \"I turned--had lent herself in an unusual degree to the display of his close grayish beard--as if he had the donkey. \"There were days when I\n",
      "Training took 2.56 minutes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "from gpt.gpt_model import GPTModel\n",
    "\n",
    "start_time = time.time()\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Vocabulary size\n",
    "    \"context_length\": 256,  # Context length !! Let's use 256 context length for faster training\n",
    "    \"embedding_dim\": 768,  # Embedding dimension\n",
    "    \"n_heads\": 12,  # Number of attention heads\n",
    "    \"n_layers\": 12,  # Number of layers\n",
    "    \"dropout_rate\": 0.1,  # Dropout rate\n",
    "    \"qkv_bias\": False,  # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=num_epochs,\n",
    "    eval_freq=5,\n",
    "    eval_iter=5,\n",
    "    start_context=\"Every effort moves you\",\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training took {execution_time_minutes:.2f} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
