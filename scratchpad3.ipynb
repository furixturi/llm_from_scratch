{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchpad 3 - Pre-train LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calculate text generation loss: cross-entropy and perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Cross-entropy loss (average negative log probability value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model generates logits tensor of (batch_size, seq_length, vocab_size), the last dimension can be interpreted as the mode's predicted probability of each token in the whole vacabulary of being the next token. Applying `softmax` makes them probability values from 0 to 1, sumning up to 1.\n",
    "\n",
    "For the target tokens (the input tokens minus the first token and plus the real next token in the training data), we want to maximize their logits in the output tokens. Which means we want to\n",
    "-  **maximize the target tokens' softmax-ed values in the ouput tensor to 1**. \n",
    "\n",
    "It's easier to do this if we first calculate their log values, because to max the probabilities to 1, then we just need to maximize their log values (which are negative numbers) to 0. \n",
    "\n",
    "In ML we need to define a loss function and minimize one loss value calculated from the function. So instead of maximizing muliple log values of all the batches' ouput token sequences, we define the loss to be the \n",
    "- **averaged negative log probability value** of all target tokens in the output tensor, from all batches in the output concatenated. \n",
    "\n",
    "This is our **cross-entropy** loss. PyTorch provides a `cross_entropy` function which combines two operations:\n",
    "- apply softmax \n",
    "- calculate log of softmax\n",
    "- calculate negative log-likelihood loss\n",
    "- average loss across batches\n",
    "\n",
    "To use it, we need to flatten the logits tensor and the target token IDs batch tensor (aka concat all batches):\n",
    "\n",
    "```\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, target_idx_batch_flat)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt.gpt_model import GPTModel\n",
    "import torch\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"embedding_dim\": 768,  # Embedding dimension\n",
    "    \"n_heads\": 12,  # Number of attention heads\n",
    "    \"n_layers\": 12,  # Number of layers\n",
    "    \"dropout_rate\": 0.1,  # Dropout rate\n",
    "    \"qkv_bias\": False,  # Query-Key-Value bias\n",
    "}\n",
    "\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full text tokens: [464, 2068, 7586, 21831, 18045, 625, 262, 16931, 3290]\n"
     ]
    }
   ],
   "source": [
    "full_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "full_text_tokens = tokenizer.encode(full_text)\n",
    "print(f\"Full text tokens: {full_text_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: The quick brown fox\n",
      "Input idx batch tensor: tensor([[  464,  2068,  7586, 21831]])\n"
     ]
    }
   ],
   "source": [
    "input_idx = full_text_tokens[:4]\n",
    "input_text = tokenizer.decode(input_idx)\n",
    "print(f\"Input text: {input_text}\")\n",
    "input_idx_batch = torch.tensor(input_idx).unsqueeze(0)\n",
    "print(f\"Input idx batch tensor: {input_idx_batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output logits shape: torch.Size([1, 4, 50257])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(input_idx_batch)\n",
    "print(f\"Output logits shape: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target text:  quick brown fox jumps\n",
      "Target idx batch tensor: tensor([[ 2068,  7586, 21831, 18045]])\n"
     ]
    }
   ],
   "source": [
    "target_idx = full_text_tokens[1:5]\n",
    "target_text = tokenizer.decode(target_idx)\n",
    "print(f\"Target text: {target_text}\")\n",
    "target_idx_batch = torch.tensor(target_idx).unsqueeze(0)\n",
    "print(f\"Target idx batch tensor: {target_idx_batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.7479)\n"
     ]
    }
   ],
   "source": [
    "# logits is the model's output of shape (batch_size, seq_len, vocab_size) -> (batch_size * seq_len, vocab_size)\n",
    "logits_flat = logits.flatten(0, 1)\n",
    "# targets is the target sequence's token IDs shape (batch_size, seq_len) -> (batch_size * seq_len)\n",
    "targets_idx_flat = target_idx_batch.flatten(0, 1)\n",
    "# Compute the cross entropy loss\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_idx_flat)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our randomly initialized GPT2 model, the cross-entropy loss is 10.7479."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perplexity** is also a metric that measures how well the model predicts a sequence of tokens. It is the **exponential of the average negaltive log-likelihood** of the predicted probabilities for each token in a sequence, aka\n",
    "- the **exponential of cross-entropy**\n",
    "  \n",
    "It tells us how \"surprised\" the model is by the test data. It can be interpreted as how many choices the model thinks it has when predicting the next token.\n",
    "\n",
    "$$\\text{Perplexity} = \\exp\\left(-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(x_i)\\right)$$\n",
    "\n",
    "```\n",
    "cros_entropy_loss = torch.nn.functional.cross_entropy(logits_flat, target_idx_batch_flat)\n",
    "perplexity_loss = torch.exp(cros_entropy_loss)\n",
    "```\n",
    "\n",
    "- A perplexity of 1 means the model is extremely sure (assigns probability of 1 to the correct next token ID).\n",
    "- A high perplexity means the model is uncertain about the next token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(46531.8984)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our untrained model has perplexity of 46532, with the vocabulary size 50257, it is basically random guessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
